name: Scheduled Benchmark

on:
  schedule:
    - cron: '0 5 * * *'  # Daily at 05:00 UTC
  workflow_dispatch: {}

jobs:
  benchmark:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install package (bench extras)
        run: |
          python -m pip install --upgrade pip
          pip install .[bench]

      - name: Run synthetic benchmark (bootstrap CIs)
        run: |
          python -m benchmarks.run_benchmark --dataset synthetic --queries 30 --k 10 --m 400 \
            --bootstrap --boots 300 --output bench.md --json bench.json --no-api

      - name: Display benchmark summary
        run: cat bench.md
      - name: Validate metrics JSON
        run: |
          python - <<'PY'
          import json, sys
          with open('bench.json','r',encoding='utf-8') as f:
              data = json.load(f)
          methods = data.get('methods', [])
          required_keys = ['nDCG@10', 'MRR@10', 'Recall@10', 'MAP@10']
          if not methods:
              print('No methods in benchmark output', file=sys.stderr); sys.exit(1)
          baseline = methods[0]
          missing = [k for k in required_keys if k not in baseline]
          if missing:
              print('Missing baseline metrics:', missing, file=sys.stderr); sys.exit(1)
          print('Benchmark validation passed.')
          PY

      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results
          path: |
            bench.md
            bench.json
